{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f023618-c9bf-4689-9912-af28f33c00b5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cafc276-869b-4e38-a61c-3a03a14c990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.layers import Norm\n",
    "from monai.data import CacheDataset, decollate_batch\n",
    "import numpy as np\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import (Transform,AsDiscrete,Activations, Activationsd, Compose, LoadImaged,\n",
    "                              Transposed, ScaleIntensityd, RandAxisFlipd, RandRotated, RandAxisFlipd,\n",
    "                              RandBiasFieldd, ScaleIntensityRangePercentilesd, RandAdjustContrastd,\n",
    "                              RandHistogramShiftd, DivisiblePadd, Orientationd, RandGibbsNoised, Spacingd,\n",
    "                              RandRicianNoised, AsChannelLastd, RandSpatialCropd,ToNumpyd,EnsureChannelFirstd,\n",
    "                              RandSpatialCropSamplesd, RandCropByPosNegLabeld)\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import nibabel as nib\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pytorch_lightning\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2093c07-6e77-4ba4-a528-2ffb93583c9e",
   "metadata": {},
   "source": [
    "## Wandb login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c189b28-b862-4135-9f61-ac742c43f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malijohnnaqvi6\u001b[0m (\u001b[33mali-john\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb353c-801c-4310-a3ab-bd8dbba5e4e1",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b75f845-a70e-42e3-a1a3-a27206d217a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        \n",
    "        LoadImaged(keys=[\"img\"],image_only=True),\n",
    "        EnsureChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"],\n",
    "            minv=0.0,\n",
    "            maxv=1.0),\n",
    "        RandRotated(keys=[\"img\"],\n",
    "            range_x=np.pi / 12,\n",
    "            prob=0.5,\n",
    "            keep_size=True,\n",
    "            mode=\"nearest\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"],image_only=True),\n",
    "        EnsureChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=[\"img\"],\n",
    "            minv=0.0,\n",
    "            maxv=1.0),\n",
    "        RandRotated(keys=[\"img\"],\n",
    "            range_x=np.pi / 12,\n",
    "            prob=0.5,\n",
    "            keep_size=True,\n",
    "            mode=\"nearest\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(batch, root_dir):\n",
    "   \n",
    "    data = pd.read_csv(os.path.join(root_dir,'participants_csv.csv'))\n",
    "    imgs_list = list(data['participant_id'])\n",
    "    age_labels = list(data['age'])\n",
    "    print(imgs_list[0])\n",
    "    \n",
    "\n",
    "    length = len(imgs_list)\n",
    "    print(f'Total images: {length}')\n",
    "    # why we are using 85 of dataset?? we can split 100% of dataset\n",
    "    test = int(0.85*length)\n",
    "\n",
    "    imgs_list = imgs_list[:test]\n",
    "    age_labels = age_labels[:test]\n",
    "\n",
    "    first = int(0.75*length)\n",
    "\n",
    "    imgs_list_train = imgs_list[0:first]\n",
    "    imgs_list_val = imgs_list[first:]\n",
    "    age_labels_train = age_labels[0:first]\n",
    "    age_labels_val = age_labels[first:]\n",
    "\n",
    "    print('train set', len(imgs_list_train), len(age_labels_train))\n",
    "    print('val set', len(imgs_list_val), len(age_labels_val))\n",
    "\n",
    "    filenames_train = [{\"img\": x, \"age\": y} for (x,y) in zip(imgs_list_train, age_labels_train)]\n",
    "\n",
    "\n",
    "    ds_train = monai.data.Dataset(filenames_train, train_transforms)\n",
    "    print('ds train type', type(ds_train))\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch, shuffle = True, num_workers=2, pin_memory=True, collate_fn=pad_list_data_collate)\n",
    "\n",
    "    filenames_val = [{\"img\": x, \"age\": y} for (x, y) in zip(imgs_list_val, age_labels_val)]\n",
    "\n",
    "    ds_val = monai.data.Dataset(filenames_val, val_transforms)\n",
    "    print('ds val type', type(ds_val))\n",
    "    val_loader = DataLoader(ds_val, batch_size=batch, shuffle=True, num_workers=1, pin_memory=True, collate_fn=pad_list_data_collate)\n",
    "    \n",
    "    return ds_train, train_loader, ds_val, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb872893-ce33-4ba2-994e-4891f06b90a3",
   "metadata": {},
   "source": [
    "## 3D U-net Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9d1c19-6d09-45f3-bd75-5f0d459783f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class build_seq_sfcn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, kernel_size=3, padding=1), #[2,32,256,256,192]\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.MaxPool3d((2, 2, 2)),#[2,32,128,128,96]\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),#[2,64,128,128,96]\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.MaxPool3d((2, 2, 2)),#[2,64,64,64,48]\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),#[2,128,64,64,48]\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.MaxPool3d((2, 2, 2)),#[2,128,32,32,24]\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),#[2,256,32,32,24]\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.MaxPool3d((2, 2, 2)),#[2,256,16,16,12]\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),#[2,256,16,16,12]\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.MaxPool3d((2, 2, 2)),#[2,256,8,8,6]\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv3d(256, 64, kernel_size=1, padding=1),#[2,64,8,8,6]\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.regressor= nn.Sequential(\n",
    "            nn.Conv3d(64, 1, kernel_size=1, padding=1),#[2,1,8,8,6]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1440, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        glob_age_output = self.features(inputs)\n",
    "        glob_age_output = self.regressor(glob_age_output)\n",
    "\n",
    "\n",
    "        return glob_age_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470e8b0-0273-4684-b5d2-15924d0e4eac",
   "metadata": {},
   "source": [
    "## Train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bceaae23-4352-4d66-8c63-124fa550979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optimizer, scheduler, max_epochs, root_dir):\n",
    "\n",
    "    metrices = {}\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('Device set to Cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model.train()\n",
    "    loss_object  = nn.MSELoss()\n",
    "    best_val_loss = 0.0\n",
    "    best_mae_score = 0.0\n",
    "    for epoch in range(1,max_epochs +1):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "    \n",
    "        print(\"Epoch \", epoch)\n",
    "        print(\"Train:\", end =\"\")\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            img, age = (batch[\"img\"].cuda(), batch[\"age\"].cuda())\n",
    "            age = age.unsqueeze(1)\n",
    "       \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_glob_age = model(img)\n",
    "\n",
    "            loss = loss_object(pred_glob_age.float(), age.float())\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"=\", end = \"\")\n",
    "\n",
    "        train_loss = train_loss/(step+1)\n",
    "        metrices[\"train_loss\"] = train_loss\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print(\"Val:\", end =\"\")\n",
    "        with torch.no_grad():\n",
    "                mae_loss=0.0\n",
    "                for step, batch in enumerate(val_loader):\n",
    "                    img, age = (batch[\"img\"].cuda(), batch[\"age\"].cuda())\n",
    "                    age = age.unsqueeze(1)\n",
    "\n",
    "                    pred_glob_age = model(img)\n",
    "\n",
    "\n",
    "                    loss = loss_object(pred_glob_age.float(), age.float())\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    print(\"=\", end = \"\")\n",
    "                print()\n",
    "                val_loss = val_loss/(step+1)\n",
    "                metrices[\"val_loss\"] = val_loss\n",
    "\n",
    "\n",
    "        print(\"Training epoch \", epoch, \", train loss:\", train_loss, \", val loss:\", val_loss, \" | \", optimizer.param_groups[0]['lr'])\n",
    "        wandb.log(metrices)\n",
    "        if epoch == 1:\n",
    "            best_val_loss = val_loss\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"Saving model\")\n",
    "            best_val_loss = val_loss\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(state, '/home/wamika/ml_project/models/global_age/' + f\"model_{epoch}.pth\")\n",
    "        scheduler.step()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896743d6-9227-4be0-a771-fc9c5e698c65",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c6667-4dde-448b-8e59-dfff13eb25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wamika/ml_project/anat/sub-CC110033/anat/sub-CC110033_T1w.nii.gz\n",
      "Total images: 653\n",
      "train set 489 489\n",
      "val set 66 66\n",
      "ds train type <class 'monai.data.dataset.Dataset'>\n",
      "ds val type <class 'monai.data.dataset.Dataset'>\n",
      "device set to Cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wamika/ml_project/code/wandb/run-20240325_131539-u7pktbqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ali-john/Global%20age%20prediction/runs/u7pktbqb' target=\"_blank\">flowing-durian-1</a></strong> to <a href='https://wandb.ai/ali-john/Global%20age%20prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ali-john/Global%20age%20prediction' target=\"_blank\">https://wandb.ai/ali-john/Global%20age%20prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ali-john/Global%20age%20prediction/runs/u7pktbqb' target=\"_blank\">https://wandb.ai/ali-john/Global%20age%20prediction/runs/u7pktbqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training...\n",
      "Device set to Cuda\n",
      "Epoch  1\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  1 , train loss: 356.1148931526699 , val loss: 369.57370549982244  |  0.001\n",
      "Epoch  2\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  2 , train loss: 196.94310204090516 , val loss: 729.8856534090909  |  0.001\n",
      "Epoch  3\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  3 , train loss: 183.28752690882772 , val loss: 911.4110107421875  |  0.001\n",
      "Epoch  4\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  4 , train loss: 177.65944904491215 , val loss: 644.3483567671342  |  0.001\n",
      "Epoch  5\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  5 , train loss: 162.753443238194 , val loss: 389.7922099720348  |  0.001\n",
      "Epoch  6\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  6 , train loss: 159.37833428821682 , val loss: 537.5222695090554  |  0.001\n",
      "Epoch  7\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  7 , train loss: 146.82957220370054 , val loss: 297.421065937389  |  0.001\n",
      "Saving model\n",
      "Epoch  8\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  8 , train loss: 136.18092932145288 , val loss: 328.9022085016424  |  0.001\n",
      "Epoch  9\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  9 , train loss: 124.74123104072056 , val loss: 301.538854078813  |  0.001\n",
      "Epoch  10\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  10 , train loss: 117.07231115929189 , val loss: 429.9939686168324  |  0.001\n",
      "Epoch  11\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  11 , train loss: 119.4504907760152 , val loss: 235.0717738758434  |  0.001\n",
      "Saving model\n",
      "Epoch  12\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  12 , train loss: 112.45896878564285 , val loss: 402.24947357177734  |  0.001\n",
      "Epoch  13\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  13 , train loss: 101.79397787930775 , val loss: 305.34374237060547  |  0.001\n",
      "Epoch  14\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  14 , train loss: 92.43664360631463 , val loss: 393.56202420321375  |  0.001\n",
      "Epoch  15\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  15 , train loss: 83.68267982283984 , val loss: 205.18137602372602  |  0.001\n",
      "Saving model\n",
      "Epoch  16\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  16 , train loss: 76.13748028673277 , val loss: 205.11644727533513  |  0.001\n",
      "Saving model\n",
      "Epoch  17\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  17 , train loss: 73.39379063266918 , val loss: 196.6661220030351  |  0.001\n",
      "Saving model\n",
      "Epoch  18\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  18 , train loss: 75.60568582353416 , val loss: 316.8242319280451  |  0.001\n",
      "Epoch  19\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  19 , train loss: 64.85360223054886 , val loss: 186.86620972373268  |  0.001\n",
      "Saving model\n",
      "Epoch  20\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  20 , train loss: 61.78110848175236 , val loss: 324.9360337690874  |  0.001\n",
      "Epoch  21\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  21 , train loss: 45.64871586502695 , val loss: 279.1682080355558  |  0.0005\n",
      "Epoch  22\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  22 , train loss: 39.54711363834838 , val loss: 230.00629633123225  |  0.0005\n",
      "Epoch  23\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  23 , train loss: 36.657333626337575 , val loss: 243.4261474609375  |  0.0005\n",
      "Epoch  24\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  24 , train loss: 39.62085996376225 , val loss: 185.74969200654462  |  0.0005\n",
      "Saving model\n",
      "Epoch  25\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  25 , train loss: 32.81973109874257 , val loss: 211.13535829023883  |  0.0005\n",
      "Epoch  26\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  26 , train loss: 32.9852399957692 , val loss: 231.15296433188698  |  0.0005\n",
      "Epoch  27\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  27 , train loss: 28.859972800213868 , val loss: 258.3194919932972  |  0.0005\n",
      "Epoch  28\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  28 , train loss: 31.99552353759485 , val loss: 296.5172812721946  |  0.0005\n",
      "Epoch  29\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  29 , train loss: 27.40954657858866 , val loss: 253.5175162228671  |  0.0005\n",
      "Epoch  30\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  30 , train loss: 22.228091228227672 , val loss: 153.4448757171631  |  0.0005\n",
      "Saving model\n",
      "Epoch  31\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  31 , train loss: 22.111448056310234 , val loss: 207.85007199374112  |  0.0005\n",
      "Epoch  32\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  32 , train loss: 20.96697392236967 , val loss: 259.39464256980204  |  0.0005\n",
      "Epoch  33\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  33 , train loss: 20.347365660170105 , val loss: 156.96402046897194  |  0.0005\n",
      "Epoch  34\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  34 , train loss: 21.104833464315334 , val loss: 150.99140852147883  |  0.0005\n",
      "Saving model\n",
      "Epoch  35\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  35 , train loss: 19.42695178534904 , val loss: 191.79937154596502  |  0.0005\n",
      "Epoch  36\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  36 , train loss: 20.375236275005925 , val loss: 193.91428045793012  |  0.0005\n",
      "Epoch  37\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  37 , train loss: 14.598335960160004 , val loss: 212.67590037259188  |  0.0005\n",
      "Epoch  38\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  38 , train loss: 13.779745519161224 , val loss: 182.22798746282405  |  0.0005\n",
      "Epoch  39\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  39 , train loss: 12.947189100658052 , val loss: 235.55111208829013  |  0.0005\n",
      "Epoch  40\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  40 , train loss: 16.519137398962595 , val loss: 196.04624964974144  |  0.0005\n",
      "Epoch  41\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  41 , train loss: 9.794146879427592 , val loss: 185.72884698347613  |  0.00025\n",
      "Epoch  42\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  42 , train loss: 9.955369048757603 , val loss: 180.2171742699363  |  0.00025\n",
      "Epoch  43\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  43 , train loss: 8.06371082943447 , val loss: 162.08275716955012  |  0.00025\n",
      "Epoch  44\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  44 , train loss: 7.636514096263728 , val loss: 188.3510060743852  |  0.00025\n",
      "Epoch  45\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  45 , train loss: 7.705726819352869 , val loss: 203.09825125607577  |  0.00025\n",
      "Epoch  46\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  46 , train loss: 8.153962251416013 , val loss: 173.21583687175405  |  0.00025\n",
      "Epoch  47\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  47 , train loss: 9.649746497853036 , val loss: 141.91522004387596  |  0.00025\n",
      "Saving model\n",
      "Epoch  48\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  48 , train loss: 8.2406753525595 , val loss: 158.6140240755948  |  0.00025\n",
      "Epoch  49\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  49 , train loss: 7.094716331344441 , val loss: 177.38451073386452  |  0.00025\n",
      "Epoch  50\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  50 , train loss: 8.683024095062828 , val loss: 198.36159446022728  |  0.00025\n",
      "Epoch  51\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  51 , train loss: 6.250053517109046 , val loss: 172.05870853770864  |  0.00025\n",
      "Epoch  52\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  52 , train loss: 8.870855143053765 , val loss: 187.5060873898593  |  0.00025\n",
      "Epoch  53\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  53 , train loss: 6.972221957896751 , val loss: 128.40863557295367  |  0.00025\n",
      "Saving model\n",
      "Epoch  54\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  54 , train loss: 6.684000655323084 , val loss: 170.46008638902143  |  0.00025\n",
      "Epoch  55\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  55 , train loss: 8.699471322945291 , val loss: 178.71577141501686  |  0.00025\n",
      "Epoch  56\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  56 , train loss: 6.41011990798763 , val loss: 180.4586417458274  |  0.00025\n",
      "Epoch  57\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  57 , train loss: 6.292750139223652 , val loss: 148.03266074440697  |  0.00025\n",
      "Epoch  58\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  58 , train loss: 6.270635552567207 , val loss: 153.48585232821378  |  0.00025\n",
      "Epoch  59\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  59 , train loss: 6.536698912413208 , val loss: 199.54420055042613  |  0.00025\n",
      "Epoch  60\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  60 , train loss: 5.976102571180261 , val loss: 140.31136348030785  |  0.00025\n",
      "Epoch  61\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  61 , train loss: 5.117563865850308 , val loss: 150.35909600691363  |  0.000125\n",
      "Epoch  62\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  62 , train loss: 3.9101976887580077 , val loss: 141.87318897247314  |  0.000125\n",
      "Epoch  63\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  63 , train loss: 4.206132706871793 , val loss: 153.46806942332876  |  0.000125\n",
      "Epoch  64\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  64 , train loss: 4.556160179761015 , val loss: 166.24945805289528  |  0.000125\n",
      "Epoch  65\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  65 , train loss: 4.281120547853365 , val loss: 154.33877381411466  |  0.000125\n",
      "Epoch  66\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  66 , train loss: 3.5444858366497822 , val loss: 163.7036979631944  |  0.000125\n",
      "Epoch  67\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  67 , train loss: 3.9168880659904035 , val loss: 157.44843968478116  |  0.000125\n",
      "Epoch  68\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  68 , train loss: 3.727708092016494 , val loss: 132.22574719515714  |  0.000125\n",
      "Epoch  69\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  69 , train loss: 3.613184882379337 , val loss: 158.83757157759234  |  0.000125\n",
      "Epoch  70\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  70 , train loss: 3.7185180900287045 , val loss: 151.75717223774302  |  0.000125\n",
      "Epoch  71\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  71 , train loss: 3.570436064633855 , val loss: 153.01362306421453  |  0.000125\n",
      "Epoch  72\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  72 , train loss: 2.8278498889569854 , val loss: 162.43643032420766  |  0.000125\n",
      "Epoch  73\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  73 , train loss: 3.3753248460910803 , val loss: 173.42917823791504  |  0.000125\n",
      "Epoch  74\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  74 , train loss: 3.037312988260407 , val loss: 153.61990425803444  |  0.000125\n",
      "Epoch  75\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  75 , train loss: 3.1074068034810525 , val loss: 163.9597443667325  |  0.000125\n",
      "Epoch  76\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  76 , train loss: 3.0567813427444617 , val loss: 157.1515439640392  |  0.000125\n",
      "Epoch  77\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  77 , train loss: 3.4557988485264266 , val loss: 174.7315007990057  |  0.000125\n",
      "Epoch  78\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  78 , train loss: 2.772542915186999 , val loss: 174.05502995577726  |  0.000125\n",
      "Epoch  79\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  79 , train loss: 3.0417181070596895 , val loss: 147.05337277325717  |  0.000125\n",
      "Epoch  80\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  80 , train loss: 2.5658622617187676 , val loss: 133.94641138206828  |  0.000125\n",
      "Epoch  81\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  81 , train loss: 2.6353866250877 , val loss: 162.0041640888561  |  6.25e-05\n",
      "Epoch  82\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  82 , train loss: 2.8296800229264183 , val loss: 157.46546017039907  |  6.25e-05\n",
      "Epoch  83\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  83 , train loss: 2.3262214476894014 , val loss: 153.50419634038752  |  6.25e-05\n",
      "Epoch  84\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  84 , train loss: 2.0631759391149127 , val loss: 147.1786194714633  |  6.25e-05\n",
      "Epoch  85\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  85 , train loss: 2.37101944371418 , val loss: 131.23025196248835  |  6.25e-05\n",
      "Epoch  86\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  86 , train loss: 2.0723291582376495 , val loss: 143.06539700248024  |  6.25e-05\n",
      "Epoch  87\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  87 , train loss: 2.1639101358553385 , val loss: 151.56811263344505  |  6.25e-05\n",
      "Epoch  88\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  88 , train loss: 2.357622127088667 , val loss: 159.11002471230248  |  6.25e-05\n",
      "Epoch  89\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  89 , train loss: 2.1307986530423895 , val loss: 138.67750044302508  |  6.25e-05\n",
      "Epoch  90\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  90 , train loss: 1.7825719145757997 , val loss: 152.70070613514292  |  6.25e-05\n",
      "Epoch  91\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  91 , train loss: 1.8829056286692984 , val loss: 146.5188751220703  |  6.25e-05\n",
      "Epoch  92\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  92 , train loss: 2.3924940167554865 , val loss: 163.1928573955189  |  6.25e-05\n",
      "Epoch  93\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  93 , train loss: 2.645398824675683 , val loss: 155.5506810274991  |  6.25e-05\n",
      "Epoch  94\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  94 , train loss: 1.9323093284812807 , val loss: 139.44299832257357  |  6.25e-05\n",
      "Epoch  95\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  95 , train loss: 1.9635545742338054 , val loss: 143.25503037192604  |  6.25e-05\n",
      "Epoch  96\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  96 , train loss: 2.5798251965980223 , val loss: 163.88362364335492  |  6.25e-05\n",
      "Epoch  97\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  97 , train loss: 1.8020025802193427 , val loss: 150.62930540605024  |  6.25e-05\n",
      "Epoch  98\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  98 , train loss: 1.7389803424568995 , val loss: 139.1540274186568  |  6.25e-05\n",
      "Epoch  99\n",
      "Train:===================================================================================================================================================================\n",
      "Val:======================\n",
      "Training epoch  99 , train loss: 2.5933919441571995 , val loss: 157.30069212480024  |  6.25e-05\n",
      "Epoch  100\n",
      "Train:==================================================================================================================================================="
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 3\n",
    "    epochs = 100\n",
    "    root_dir = '/home/wamika/ml_project/anat/'\n",
    "    \n",
    "\n",
    "\n",
    "    ds_train, train_loader, ds_val, val_loader = load_data( batch_size, root_dir)\n",
    "    \n",
    "    # Building our 3D UNET model\n",
    "    model = build_seq_sfcn()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('device set to Cuda')\n",
    "\n",
    "    else:\n",
    "        print (\"Cuda not found\")\n",
    "        device= torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    step = 20\n",
    "    gamma=0.5\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=1e-5, betas=(0.5, 0.999))\n",
    "    scheduler = StepLR(optimizer, step_size=step, gamma=gamma)\n",
    "    wandb.init(\n",
    "    project=\"Global age prediction\",\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"SC-FN\",\n",
    "    \"dataset\": \"CAMCAN\",\n",
    "    \"epochs\": 100,\n",
    "    }\n",
    ")\n",
    "    \n",
    "    print(\"Start of training...\")\n",
    "    train(train_loader, val_loader, model, optimizer, scheduler, epochs, root_dir)\n",
    "    print(\"End of training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ffea9-8126-4353-97d8-8500303c0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/wamika/ml_project/train.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    for item in train_losses:\n",
    "        file.write(str(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb1f4b-dd2f-4a78-b73b-1131a505ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/home/wamika/ml_project/val.txt\"\n",
    "with open(p, \"w\") as f:\n",
    "    for item in val_losses:\n",
    "        f.write(str(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12bb6d-9b81-4f37-8a7a-b399e46418c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
